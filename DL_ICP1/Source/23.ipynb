{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLa6LCjKn2hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the data source to Breast Cancerdataset * available in the source folder and make required changes\n",
        "# Normalize the data before feeding the data to the model and check how the normalization change your accuracy (code given below).\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc =StandardScaler()\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "import pandas as pd\n",
        "\n",
        "# Loading data using pandas\n",
        "dataset = pd.read_csv(\"Breas Cancer.csv\")\n",
        "\n",
        "X = dataset.iloc[:, 2:32].values\n",
        "y = dataset.iloc[:, 1].values\n",
        "print(dataset.iloc[:, 1].value_counts())\n",
        "\n",
        "# Applying Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "y = labelencoder_X_1.fit_transform(y) \n",
        "\n",
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "# Apply sequential model\n",
        "my_first_nn = Sequential() \n",
        "\n",
        "# Add dense layers\n",
        "my_first_nn.add(Dense(20, input_dim=30, activation='relu')) \n",
        "my_first_nn.add(Dense(1, activation='sigmoid')) \n",
        "my_first_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "my_first_nn_fitted = my_first_nn.fit(X_train, y_train, epochs=100, verbose=0, initial_epoch=0)\n",
        "\n",
        "# Display before normalization\n",
        "print(my_first_nn.summary())\n",
        "print(my_first_nn.evaluate(X_test, y_test))\n",
        "\n",
        "# Apply Standard Scaler featute\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit and Transform data\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "# Apply sequential model\n",
        "my_first_nn1 = Sequential() \n",
        "my_first_nn1.add(Dense(20, input_dim=30, activation='relu'))\n",
        "my_first_nn1.add(Dense(1, activation='sigmoid')) \n",
        "my_first_nn1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "my_first_nn_fitted1 = my_first_nn1.fit(X_train, y_train, epochs=100, verbose=0, initial_epoch=0)\n",
        "\n",
        "# Display after Normalization\n",
        "print(\"\\n----------Normalization Changes---------------\\n\")\n",
        "print(\"Summary:\",my_first_nn.summary())\n",
        "print(\"Evaluate:\",my_first_nn.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}